{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Test Spark to make sure it works delete later.\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "# do something to prove it works\n",
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.takeSample(False, 5)\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+-----------+--------------------+-------------------+-------------+------------+----------+---------+---------+--------+---------+--------+----------+\n",
      "|           status_id|      status_message| link_name|status_type|         status_link|   status_published|num_reactions|num_comments|num_shares|num_likes|num_loves|num_wows|num_hahas|num_sads|num_angrys|\n",
      "+--------------------+--------------------+----------+-----------+--------------------+-------------------+-------------+------------+----------+---------+---------+--------+---------+--------+----------+\n",
      "|160558090672531_1...|So, rain is in th...|      null|     status|                null|2016-12-22 13:34:30|            4|          15|         0|        4|        0|       0|        0|       0|         0|\n",
      "|160558090672531_1...|                null|      null|     status|                null|2016-12-22 21:38:02|           15|           3|         0|       14|        0|       1|        0|       0|         0|\n",
      "|160558090672531_1...|              Zoomie|      null|      video|https://www.faceb...|2016-12-21 19:12:13|           55|          13|         4|       50|        0|       5|        0|       0|         0|\n",
      "|160558090672531_1...|Someone don't see...|      null|      video|https://www.faceb...|2016-12-21 17:40:37|           34|           5|         3|       34|        0|       0|        0|       0|         0|\n",
      "|160558090672531_1...|Know what would m...|      null|     status|                null|2016-12-21 09:51:47|           12|          13|         0|       12|        0|       0|        0|       0|         0|\n",
      "|160558090672531_1...|                null|      null|      video|https://www.faceb...|2016-12-21 19:04:09|           19|           7|         3|       15|        0|       4|        0|       0|         0|\n",
      "|160558090672531_1...|So, I plan on bei...|      null|     status|                null|2016-12-20 12:09:08|            4|           2|         0|        4|        0|       0|        0|       0|         0|\n",
      "|160558090672531_1...|               Johno|      null|      video|https://www.faceb...|2016-12-21 18:25:39|           19|           1|         4|       19|        0|       0|        0|       0|         0|\n",
      "|160558090672531_1...|        Shawn Malone|      null|      video|https://www.faceb...|2016-12-21 18:54:30|           14|           4|         1|       13|        1|       0|        0|       0|         0|\n",
      "|160558090672531_1...|                null|      null|      video|https://www.faceb...|2016-12-21 18:58:56|           13|           8|         0|       13|        0|       0|        0|       0|         0|\n",
      "|160558090672531_1...|Doopey, Mr. Magoo...|      null|      photo|https://www.faceb...|2016-12-21 18:35:29|            9|           0|         0|        9|        0|       0|        0|       0|         0|\n",
      "|160558090672531_1...|          Chris King|      null|      video|https://www.faceb...|2016-12-21 17:53:35|           13|           4|         0|       13|        0|       0|        0|       0|         0|\n",
      "|160558090672531_1...|we needed more sa...|      null|      photo|https://www.faceb...|2016-12-21 18:25:58|           11|           0|         0|       11|        0|       0|        0|       0|         0|\n",
      "|160558090672531_1...|        safety break|      null|      photo|https://www.faceb...|2016-12-21 18:25:12|            5|           0|         0|        5|        0|       0|        0|       0|         0|\n",
      "|160558090672531_1...|Small group of li...|      null|      video|https://www.faceb...|2016-12-21 18:16:21|           15|           0|         0|       15|        0|       0|        0|       0|         0|\n",
      "|160558090672531_1...|    Mr. Randy Barnes|      null|      video|https://www.faceb...|2016-12-21 17:59:13|           17|           0|         0|       17|        0|       0|        0|       0|         0|\n",
      "|160558090672531_1...|Just dropped on t...|ThreeWaves|      photo|https://www.faceb...|2016-12-21 16:27:25|            2|           0|         0|        2|        0|       0|        0|       0|         0|\n",
      "|160558090672531_1...|Thinking of bring...|      null|     status|                null|2016-12-20 14:45:04|            2|          10|         0|        2|        0|       0|        0|       0|         0|\n",
      "|160558090672531_1...|                null|      null|     status|                null|2016-12-21 11:28:42|           32|           6|         0|       31|        1|       0|        0|       0|         0|\n",
      "|160558090672531_1...|When you're tryin...|      null|      video|https://www.faceb...|2016-12-20 11:40:42|          104|          16|         3|       99|        0|       2|        3|       0|         0|\n",
      "+--------------------+--------------------+----------+-----------+--------------------+-------------------+-------------+------------+----------+---------+---------+--------+---------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|      status_message|            filtered|\n",
      "+--------------------+--------------------+\n",
      "|So, rain is in th...|[so,, rain, is, i...|\n",
      "|              Zoomie|            [zoomie]|\n",
      "|Someone don't see...|[someone, don't, ...|\n",
      "|Know what would m...|[know, what, woul...|\n",
      "|So, I plan on bei...|[so,, i, plan, on...|\n",
      "|               Johno|             [johno]|\n",
      "|        Shawn Malone|     [shawn, malone]|\n",
      "|Doopey, Mr. Magoo...|[doopey,, mr., ma...|\n",
      "|          Chris King|       [chris, king]|\n",
      "|we needed more sa...|[we, needed, more...|\n",
      "|        safety break|     [safety, break]|\n",
      "|Small group of li...|[small, group, of...|\n",
      "|    Mr. Randy Barnes|[mr., randy, barnes]|\n",
      "|Just dropped on t...|[just, dropped, o...|\n",
      "|Thinking of bring...|[thinking, of, br...|\n",
      "|When you're tryin...|[when, you're, tr...|\n",
      "|https://youtu.be/...|[https://youtu.be...|\n",
      "|https://youtu.be/...|[https://youtu.be...|\n",
      "|one of the best o...|[one, of, the, be...|\n",
      "|Move pretty damn ...|[move, pretty, da...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|      status_message|            filtered|           filtered1|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|So, rain is in th...|[so,, rain, is, i...|[so,, rain, forec...|\n",
      "|              Zoomie|            [zoomie]|            [zoomie]|\n",
      "|Someone don't see...|[someone, don't, ...|[someone, don't, ...|\n",
      "|Know what would m...|[know, what, woul...|[know, would, mak...|\n",
      "|So, I plan on bei...|[so,, i, plan, on...|[so,, plan, knoxv...|\n",
      "|               Johno|             [johno]|             [johno]|\n",
      "|        Shawn Malone|     [shawn, malone]|     [shawn, malone]|\n",
      "|Doopey, Mr. Magoo...|[doopey,, mr., ma...|[doopey,, mr., ma...|\n",
      "|          Chris King|       [chris, king]|       [chris, king]|\n",
      "|we needed more sa...|[we, needed, more...|    [needed, safety]|\n",
      "|        safety break|     [safety, break]|     [safety, break]|\n",
      "|Small group of li...|[small, group, of...|[small, group, li...|\n",
      "|    Mr. Randy Barnes|[mr., randy, barnes]|[mr., randy, barnes]|\n",
      "|Just dropped on t...|[just, dropped, o...|[dropped, link, k...|\n",
      "|Thinking of bring...|[thinking, of, br...|[thinking, bringi...|\n",
      "|When you're tryin...|[when, you're, tr...|[you're, trying, ...|\n",
      "|https://youtu.be/...|[https://youtu.be...|[https://youtu.be...|\n",
      "|https://youtu.be/...|[https://youtu.be...|[https://youtu.be...|\n",
      "|one of the best o...|[one, of, the, be...|[one, best, open,...|\n",
      "|Move pretty damn ...|[move, pretty, da...|[move, pretty, da...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "#sc = pyspark.SparkContext('local[*]')\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('160558090672531_facebook_statuses.csv')\n",
    "df.show()\n",
    "df = df.na.drop(subset=[\"status_message\"])\n",
    "messages = df.select(\"status_message\")\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"status_message\", outputCol=\"filtered\")\n",
    "filterw = tokenizer.transform(messages)\n",
    "filterw.show()\n",
    "remover = StopWordsRemover(inputCol=\"filtered\", outputCol=\"filtered1\")\n",
    "filtered_final = remover.transform(filterw)\n",
    "filtered_final.show()\n",
    "messages = filtered_final.select(\"filtered1\")\n",
    "message_rdd=messages.rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 81.0 failed 1 times, most recent failure: Lost task 0.0 in stage 81.0 (TID 98, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1792, in combineLocally\n    merger.mergeValues(iterator)\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 238, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1886)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1899)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1792, in combineLocally\n    merger.mergeValues(iterator)\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 238, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a43f2e8ac40f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstatuses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmessage_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwords_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstatuses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m              \u001b[1;33m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mwords_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.3-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\lib\\py4j-0.10.3-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 81.0 failed 1 times, most recent failure: Lost task 0.0 in stage 81.0 (TID 98, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1792, in combineLocally\n    merger.mergeValues(iterator)\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 238, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1886)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1899)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1792, in combineLocally\n    merger.mergeValues(iterator)\n  File \"C:\\Users\\img\\Documents\\spark-2.0.2-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 238, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\nTypeError: unhashable type: 'list'\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "statuses = message_rdd.flatMap(lambda x: x)\n",
    "classic = statuses.map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(lambda a, b: a + b)\n",
    "print(classic.collectAsMap())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'message_rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-527cf249154c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mstatuses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmessage_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mstatuses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstatuses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatuses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'message_rdd' is not defined"
     ]
    }
   ],
   "source": [
    "#Irrelvant\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "import re\n",
    "import sys\n",
    "from operator import add\n",
    "import string \n",
    "\n",
    "def word_tokenize(x):\n",
    "    return nltk.word_tokenize(x)\n",
    "\n",
    "    return x \n",
    "def shit(word):\n",
    "    l = word_tokenize(word)\n",
    "    return filtered_words\n",
    "def garbage_removal(y):\n",
    "    print(\"hello this is\" + y)\n",
    "    return y\n",
    "    #if y not in stopwords.words('english') and y not in string.punctutation:\n",
    "        #return y\n",
    "   \n",
    "    \n",
    "    #eturn x.replace(\"\"\"([\\p{Punct}&&[^.]]|\\b\\p{IsLetter}{1,2}\\b)\\s*\"\"\", \"\")\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "statuses = message_rdd.flatMap(lambda x: x)\n",
    "statuses = statuses.filter(lambda x: x is not None)\n",
    "print(statuses.take(5))\n",
    "words = statuses.map(lambda x: shit(x))\n",
    "words2 = words.flatMap(lambda x: x)\n",
    "\n",
    "print(words2.take(5))\n",
    "\n",
    "filtered_words.take(5)\n",
    "\n",
    "print(filtered_words.take(5))\n",
    "\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
