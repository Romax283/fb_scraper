{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'Queue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c01b0a0fde51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-c01b0a0fde51>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mthreads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQueue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[1;31m# Set up spark objects and run\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0msc\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'local[4]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Social Panic Analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'Queue'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import findspark\n",
    "findspark.init()\n",
    "import sys\n",
    "import ast\n",
    "import json\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import threading\n",
    "from multiprocessing import Queue\n",
    "import time\n",
    "\n",
    "#import cartopy.crs as ccrs\n",
    "\n",
    "\n",
    "\n",
    "# File with OAuth keys\n",
    "# Corresponding format is the following\n",
    "# auth = requests_oauthlib.OAuth1(key1, key2, key3, key4)\n",
    "# url = 'https://stream.twitter.com/1.1/statuses/filter.json'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BATCH_INTERVAL = 10  # How frequently to update (seconds)\n",
    "BLOCKSIZE = 50  # How many tweets per update\n",
    "\n",
    "\n",
    "def main():\n",
    "    threads = []\n",
    "    q = Queue.Queue()\n",
    "    # Set up spark objects and run\n",
    "    sc  = SparkContext('local[4]', 'Social Panic Analysis')\n",
    "    ssc = StreamingContext(sc, BATCH_INTERVAL)\n",
    "    threads.append(threading.Thread(target=spark_stream, args=(sc, ssc, q)))\n",
    "    threads.append(threading.Thread(target=data_plotting, args=(q,)))\n",
    "    [t.start() for t in threads]\n",
    "\n",
    "\n",
    "def data_plotting(q):\n",
    "    plt.ion() # Interactive mode\n",
    "    fig = plt.figure(figsize=(30, 30))\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    ax.set_extent([-130, -60, 20, 50])\n",
    "    ax.coastlines()\n",
    "    while True:\n",
    "        if q.empty():\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            data = np.array(q.get())\n",
    "            try:\n",
    "                ax.scatter(data[:, 0], data[:, 1], transform=ccrs.PlateCarree())\n",
    "                plt.draw()\n",
    "            except IndexError: # Empty array\n",
    "                pass\n",
    "\n",
    "\n",
    "def spark_stream(sc, ssc, q):\n",
    "\n",
    "    # Setup Stream\n",
    "    rdd = ssc.sparkContext.parallelize([0])\n",
    "    stream = ssc.queueStream([], default=rdd)\n",
    "\n",
    "    stream = stream.transform(tfunc)\n",
    "\n",
    "    # Analysis\n",
    "    coord_stream = stream.map(lambda line: ast.literal_eval(line)) \\\n",
    "                        .filter(filter_posts) \\\n",
    "                        .map(get_coord)\n",
    "\n",
    "    # Convert to something usable....\n",
    "    coord_stream.foreachRDD(lambda t, rdd: q.put(rdd.collect()))\n",
    "\n",
    "    # Run!\n",
    "    ssc.start()\n",
    "    ssc.awaitTermination()\n",
    "\n",
    "\n",
    "def stream_twitter_data():\n",
    "    data      = [('language', 'en'), ('locations', '-130,20,-60,50')]\n",
    "    query_url = config.url + '?' + '&'.join([str(t[0]) + '=' + str(t[1]) for t in data])\n",
    "    response  = requests.get(query_url, auth=config.auth, stream=True)\n",
    "    print(query_url, response) # 200 <OK>\n",
    "    count = 0\n",
    "    for line in response.iter_lines():  # Iterate over streaming tweets\n",
    "        try:\n",
    "            if count > BLOCKSIZE:\n",
    "                break\n",
    "            post     = json.loads(line.decode('utf-8'))\n",
    "            contents = [post['text'], post['coordinates'], post['place']]\n",
    "            count   += 1\n",
    "            yield str(contents)\n",
    "        except:\n",
    "            print(line)\n",
    "\n",
    "\n",
    "def tfunc(t, rdd):\n",
    "\n",
    "    return rdd.flatMap(lambda x: stream_twitter_data())\n",
    "\n",
    "\n",
    "def filter_posts(line):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis and identify tweets that indicate violence\n",
    "    :param line: list\n",
    "        List from dataset\n",
    "    \"\"\"\n",
    "    keywords  = ['riot', 'protest', 'violence', 'angry', 'sad', 'mourn', 'http']\n",
    "    for k in keywords:\n",
    "        if k in line[0]:\n",
    "            return True\n",
    "\n",
    "Teram\n",
    "def get_coord(line):\n",
    "    \"\"\"\n",
    "    Converts each object into /just/ the associated coordinates\n",
    "    :param line: list\n",
    "        List from dataset\n",
    "    \"\"\"\n",
    "    coord = tuple()\n",
    "    try:\n",
    "        if line[1] == None:\n",
    "            coord = line[2]['bounding_box']['coordinates']\n",
    "            coord = reduce(lambda agg, nxt: [agg[0] + nxt[0], agg[1] + nxt[1]], coord[0])\n",
    "            coord = tuple(map(lambda t: t / 4.0, coord))\n",
    "        else:\n",
    "            coord = tuple(line[1]['coordinates'])\n",
    "    except TypeError:\n",
    "        print(line)\n",
    "    return coord\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.exit(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
